version: '3.8'

services:
  # AI assistant backend
  backend:
    build: 
      context: ../backend
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      # Configuration
      - ENVIRONMENT=local_only
      - DATABASE_URL=sqlite:///./storage/chat.db
      - LLM_PROVIDER=ollama
      - OLLAMA_URL=http://ollama:11434
      - MCP_SERVERS_CONFIG=/app/mcp-config-local.json
      
      # RAG
      - VECTOR_DB_TYPE=chroma
      - VECTOR_DB_PATH=/app/storage/vector_db
      - EMBEDDING_PROVIDER=local
      - EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
      - CHUNK_SIZE=1000
      - CHUNK_OVERLAP=200
      
      # Privacy & offline mode
      - LOCAL_ONLY=true
      - OFFLINE_MODE=true
      - HUGGINGFACE_OFFLINE=true
      - DOWNLOAD_MODELS_ON_STARTUP=false
    volumes:
      - ../backend:/app
      - ../mcp-config-local.json:/app/mcp-config-local.json
      - backend_storage:/app/storage
    depends_on:
      - ollama

    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

  # Streamlit frontend
  frontend:
    build:
      context: ../frontend
      dockerfile: Dockerfile
    ports:
      - "8501:8501"
    environment:
      - STREAMLIT_SERVER_PORT=8501
      - STREAMLIT_SERVER_ADDRESS=0.0.0.0
      - BACKEND_API_URL=http://backend:8000
      - BACKEND_URL=http://backend:8000
      - LOCAL_ONLY=true
    volumes:
      - ../frontend:/app
    depends_on:
      - backend

    command: streamlit run app.py --server.port 8501 --server.address 0.0.0.0

        # LLM provider (Ollama)
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_HOST=0.0.0.0

    # Uncomment if you have GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]



volumes:
  ollama_data:
    driver: local
  backend_storage:
    driver: local 