#!/bin/sh

# Download models for local-only operation
# This script downloads all necessary models for offline use

set -e

echo "üöÄ Starting model download for local-only operation..."

# Wait for Ollama to be ready
echo "‚è≥ Waiting for Ollama to be ready..."
while ! wget -q --spider http://ollama:11434/api/tags; do
    echo "   Waiting for Ollama service..."
    sleep 5
done

echo "‚úÖ Ollama is ready!"

# Download chat models
echo "üì• Downloading chat models..."

echo "   ‚Ä¢ Downloading Llama 3.2 (main chat model)..."
ollama pull llama3.2:latest

echo "   ‚Ä¢ Downloading CodeLlama (for code tasks)..."
ollama pull codellama:7b

echo "   ‚Ä¢ Downloading Llama 3.2 1B (lightweight model)..."
ollama pull llama3.2:1b

# Download embedding models
echo "üì• Downloading embedding models..."

echo "   ‚Ä¢ Downloading Nomic Embed Text (for RAG)..."
ollama pull nomic-embed-text:latest

echo "   ‚Ä¢ Downloading BGE Small (alternative embeddings)..."
ollama pull bge-small:latest

# List downloaded models
echo "üìã Downloaded models:"
ollama list

echo "‚úÖ All models downloaded successfully!"
echo "üîí System is now ready for fully offline operation."

# Create a marker file to indicate models are downloaded
touch /tmp/models-downloaded

echo "üéâ Local-only setup complete!"
echo ""
echo "Available models:"
echo "  ‚Ä¢ llama3.2:latest      - Main chat model"
echo "  ‚Ä¢ codellama:7b         - Code assistance"
echo "  ‚Ä¢ llama3.2:1b          - Lightweight model"
echo "  ‚Ä¢ nomic-embed-text     - Document embeddings"
echo "  ‚Ä¢ bge-small            - Alternative embeddings"
echo ""
echo "üìñ Your AI assistant can now run completely offline!" 